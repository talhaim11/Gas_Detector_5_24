{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28eb1f0e",
   "metadata": {},
   "source": [
    "# tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c234ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Imports for TensorFlow video inference\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import threading\n",
    "import queue\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "441fa0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: TensorFlow config - paths, grid settings, thresholds (edit these)\n",
    "VIDEO_PATH_TF = r\"C:\\Users\\thaim\\Videos\\AI_LEDS\\second_dataset\\preparations\\detector_video\\30.mp4\"\n",
    "WEIGHTS_PATH_TF = r\"C:\\Users\\thaim\\Videos\\AI_LEDS\\second_dataset\\preparations\\model_output\\detector_classifier_mobilenetv2_final (3).h5\"\n",
    "OUTPUT_VIDEO_PATH_TF = r\"C:\\Users\\thaim\\Videos\\AI_LEDS\\second_dataset\\video_test_output_tf\"\n",
    "\n",
    "# Model and grid settings\n",
    "MODEL_INPUT_SIZE_TF = 224\n",
    "BOX_SIZE_TF = 448  # Larger boxes = fewer predictions = faster\n",
    "OVERLAP_PERCENT_TF = 0\n",
    "PROCESS_EVERY_N_FRAMES_TF = 5  # Process every 5th frame\n",
    "CONF_THRESHOLD_TF = 0.5\n",
    "\n",
    "# Display and output\n",
    "DISPLAY_WINDOW_TF = True\n",
    "SAVE_OUTPUT_TF = True\n",
    "DRAW_GRID_TF = True\n",
    "FONT_TF = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE_TF = 0.4\n",
    "THICKNESS_TF = 1\n",
    "\n",
    "# Color BGR tuples\n",
    "COLOR_POS_TF = (0, 200, 0)\n",
    "COLOR_NEG_TF = (0, 0, 200)\n",
    "COLOR_GRID_TF = (180, 180, 180)\n",
    "\n",
    "SEED_TF = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e12c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded from C:\\Users\\thaim\\Videos\\AI_LEDS\\second_dataset\\preparations\\model_output\\detector_classifier_mobilenetv2_final (3).h5\n"
     ]
    }
   ],
   "source": [
    "# Load TensorFlow model directly from saved file\n",
    "model_tf = keras.models.load_model(WEIGHTS_PATH_TF)\n",
    "model_tf.compile()\n",
    "print(f'✓ Model loaded from {WEIGHTS_PATH_TF}')\n",
    "\n",
    "def compute_stride(box_size, overlap_percent):\n",
    "    overlap_fraction = overlap_percent / 100.0\n",
    "    return max(1, int(box_size * (1 - overlap_fraction)))\n",
    "\n",
    "def preprocess_tf(crop, target_size):\n",
    "    # MATCH TRAINING: Use PIL + keras preprocessing (NOT cv2!)\n",
    "    # Convert BGR (cv2) to RGB then to PIL Image\n",
    "    rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(rgb)\n",
    "    # Resize using PIL (matches keras.load_img behavior)\n",
    "    pil_img = pil_img.resize((target_size, target_size), Image.BILINEAR)\n",
    "    # Convert to array using keras method (matches training exactly)\n",
    "    img_array = keras.preprocessing.image.img_to_array(pil_img)\n",
    "    img_array = img_array / 255.0\n",
    "    return img_array\n",
    "\n",
    "stride_tf = compute_stride(BOX_SIZE_TF, OVERLAP_PERCENT_TF)\n",
    "\n",
    "cap_tf = cv2.VideoCapture(VIDEO_PATH_TF)\n",
    "if not cap_tf.isOpened():\n",
    "    raise RuntimeError(f'Could not open video: {VIDEO_PATH_TF}')\n",
    "\n",
    "fps_tf = cap_tf.get(cv2.CAP_PROP_FPS) or 30\n",
    "width_tf = int(cap_tf.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height_tf = int(cap_tf.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "delay_ms_tf = int(1000 / fps_tf)\n",
    "\n",
    "writer_tf = None\n",
    "if SAVE_OUTPUT_TF and OUTPUT_VIDEO_PATH_TF:\n",
    "    fourcc_tf = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer_tf = cv2.VideoWriter(OUTPUT_VIDEO_PATH_TF, fourcc_tf, fps_tf, (width_tf, height_tf))\n",
    "\n",
    "frame_queue_tf = queue.Queue(maxsize=1)\n",
    "last_overlay_tf = None\n",
    "processed_frames_tf = 0\n",
    "\n",
    "# Worker thread does inference with BATCHED predictions (all boxes at once)\n",
    "def worker_tf():\n",
    "    global last_overlay_tf, processed_frames_tf\n",
    "    while True:\n",
    "        frame_tf = frame_queue_tf.get()\n",
    "        if frame_tf is None:\n",
    "            break\n",
    "        overlay_tf = frame_tf.copy()\n",
    "        \n",
    "        # Collect all boxes first\n",
    "        boxes_data = []\n",
    "        for y_tf in range(0, height_tf - BOX_SIZE_TF + 1, stride_tf):\n",
    "            for x_tf in range(0, width_tf - BOX_SIZE_TF + 1, stride_tf):\n",
    "                crop_tf = frame_tf[y_tf:y_tf + BOX_SIZE_TF, x_tf:x_tf + BOX_SIZE_TF]\n",
    "                if crop_tf.shape[0] == BOX_SIZE_TF and crop_tf.shape[1] == BOX_SIZE_TF:\n",
    "                    boxes_data.append((x_tf, y_tf, crop_tf))\n",
    "        \n",
    "        # Batch predict all boxes at once (MUCH faster than individual predictions)\n",
    "        if boxes_data:\n",
    "            batch_tensors = np.array([preprocess_tf(box[2], MODEL_INPUT_SIZE_TF) for box in boxes_data])\n",
    "            probs_tf = model_tf.predict(batch_tensors, verbose=0)[:, 0]\n",
    "            \n",
    "            # Draw results\n",
    "            for idx, (x_tf, y_tf, _) in enumerate(boxes_data):\n",
    "                prob_tf = probs_tf[idx]\n",
    "                is_det_tf = (prob_tf >= CONF_THRESHOLD_TF)\n",
    "                \n",
    "                if DRAW_GRID_TF:\n",
    "                    cv2.rectangle(overlay_tf, (x_tf, y_tf), (x_tf + BOX_SIZE_TF, y_tf + BOX_SIZE_TF), COLOR_GRID_TF, 1)\n",
    "                \n",
    "                color_tf = COLOR_POS_TF if is_det_tf else COLOR_NEG_TF\n",
    "                cv2.rectangle(overlay_tf, (x_tf, y_tf), (x_tf + BOX_SIZE_TF, y_tf + BOX_SIZE_TF), color_tf, 2)\n",
    "                label_tf = f\"{prob_tf*100:.1f}%\"\n",
    "                cv2.putText(overlay_tf, label_tf, (x_tf + 4, y_tf + 20), FONT_TF, FONT_SCALE_TF, color_tf, THICKNESS_TF, cv2.LINE_AA)\n",
    "        \n",
    "        last_overlay_tf = overlay_tf\n",
    "        processed_frames_tf += 1\n",
    "\n",
    "worker_thread_tf = threading.Thread(target=worker_tf, daemon=True)\n",
    "worker_thread_tf.start()\n",
    "\n",
    "frame_idx_tf = 0\n",
    "\n",
    "while True:\n",
    "    ret_tf, frame_tf = cap_tf.read()\n",
    "    if not ret_tf:\n",
    "        cap_tf.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        frame_idx_tf = 0\n",
    "        continue\n",
    "\n",
    "    if frame_idx_tf % PROCESS_EVERY_N_FRAMES_TF == 0 and frame_queue_tf.empty():\n",
    "        frame_queue_tf.put(frame_tf.copy())\n",
    "\n",
    "    overlay_tf = last_overlay_tf if last_overlay_tf is not None else frame_tf\n",
    "\n",
    "    if writer_tf is not None:\n",
    "        writer_tf.write(overlay_tf)\n",
    "    if DISPLAY_WINDOW_TF:\n",
    "        cv2.imshow('TensorFlow Model Grid Predictions', overlay_tf)\n",
    "        if cv2.waitKey(delay_ms_tf) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    frame_idx_tf += 1\n",
    "\n",
    "frame_queue_tf.put(None)\n",
    "worker_thread_tf.join(timeout=2)\n",
    "cap_tf.release()\n",
    "if writer_tf is not None:\n",
    "    writer_tf.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f'✓ TensorFlow done: processed {processed_frames_tf} inference frames')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0756c491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing frame 79/132 (1280×960)\n",
      "Processed 4 boxes - Press any key to close\n"
     ]
    }
   ],
   "source": [
    "# Test on single random frame from video - standalone (requires only Cell 2 & 3)\n",
    "import random\n",
    "\n",
    "# Load model if not already loaded\n",
    "if 'model_tf' not in dir():\n",
    "    model_tf = keras.models.load_model(WEIGHTS_PATH_TF)\n",
    "    model_tf.compile()\n",
    "    print(f'✓ Model loaded from {WEIGHTS_PATH_TF}')\n",
    "\n",
    "# Define preprocessing function - MATCHES TRAINING EXACTLY\n",
    "def preprocess_single(crop, target_size):\n",
    "    # Use PIL + keras preprocessing (NOT cv2) to match training\n",
    "    rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(rgb)\n",
    "    pil_img = pil_img.resize((target_size, target_size), Image.BILINEAR)\n",
    "    img_array = keras.preprocessing.image.img_to_array(pil_img)\n",
    "    img_array = img_array / 255.0\n",
    "    return img_array\n",
    "\n",
    "# Calculate stride\n",
    "def compute_stride_single(box_size, overlap_percent):\n",
    "    overlap_fraction = overlap_percent / 100.0\n",
    "    return max(1, int(box_size * (1 - overlap_fraction)))\n",
    "\n",
    "stride_single = compute_stride_single(BOX_SIZE_TF, OVERLAP_PERCENT_TF)\n",
    "\n",
    "cap_single = cv2.VideoCapture(VIDEO_PATH_TF)\n",
    "total_frames = int(cap_single.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width_single = int(cap_single.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height_single = int(cap_single.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "random_frame_idx = random.randint(0, total_frames - 1)\n",
    "cap_single.set(cv2.CAP_PROP_POS_FRAMES, random_frame_idx)\n",
    "ret_single, frame_single = cap_single.read()\n",
    "cap_single.release()\n",
    "\n",
    "if not ret_single:\n",
    "    raise RuntimeError(f'Could not read frame {random_frame_idx}')\n",
    "\n",
    "print(f'Testing frame {random_frame_idx}/{total_frames} ({width_single}×{height_single})')\n",
    "\n",
    "# Same grid inference as video worker\n",
    "overlay_single = frame_single.copy()\n",
    "boxes_data_single = []\n",
    "\n",
    "for y_s in range(0, height_single - BOX_SIZE_TF + 1, stride_single):\n",
    "    for x_s in range(0, width_single - BOX_SIZE_TF + 1, stride_single):\n",
    "        crop_s = frame_single[y_s:y_s + BOX_SIZE_TF, x_s:x_s + BOX_SIZE_TF]\n",
    "        if crop_s.shape[0] == BOX_SIZE_TF and crop_s.shape[1] == BOX_SIZE_TF:\n",
    "            boxes_data_single.append((x_s, y_s, crop_s))\n",
    "\n",
    "# Batch predict all boxes at once\n",
    "if boxes_data_single:\n",
    "    batch_tensors_single = np.array([preprocess_single(box[2], MODEL_INPUT_SIZE_TF) for box in boxes_data_single])\n",
    "    probs_single = model_tf.predict(batch_tensors_single, verbose=0)[:, 0]\n",
    "    \n",
    "    # Draw results\n",
    "    for idx, (x_s, y_s, _) in enumerate(boxes_data_single):\n",
    "        prob_s = probs_single[idx]\n",
    "        is_det_s = (prob_s >= CONF_THRESHOLD_TF)\n",
    "        \n",
    "        if DRAW_GRID_TF:\n",
    "            cv2.rectangle(overlay_single, (x_s, y_s), (x_s + BOX_SIZE_TF, y_s + BOX_SIZE_TF), COLOR_GRID_TF, 1)\n",
    "        \n",
    "        color_s = COLOR_POS_TF if is_det_s else COLOR_NEG_TF\n",
    "        cv2.rectangle(overlay_single, (x_s, y_s), (x_s + BOX_SIZE_TF, y_s + BOX_SIZE_TF), color_s, 2)\n",
    "        label_s = f\"{prob_s*100:.1f}%\"\n",
    "        cv2.putText(overlay_single, label_s, (x_s + 4, y_s + 20), FONT_TF, FONT_SCALE_TF, color_s, THICKNESS_TF, cv2.LINE_AA)\n",
    "\n",
    "# Display result\n",
    "cv2.imshow('Single Frame Test', overlay_single)\n",
    "print(f'Processed {len(boxes_data_single)} boxes - Press any key to close')\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26392f32",
   "metadata": {},
   "source": [
    "# torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deba88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Imports for video handling, model inference, and utilities\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import threading\n",
    "import queue\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b083dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Configurable paths, grid settings, and thresholds (edit these)\n",
    "VIDEO_PATH = r\"C:\\Users\\thaim\\Videos\\AI_LEDS\\second_dataset\\preparations\\detector_video\"\n",
    "WEIGHTS_PATH = r\"C:\\Users\\thaim\\Videos\\AI_LEDS\\first_dataset\\preparations\\model_output\"\n",
    "OUTPUT_VIDEO_PATH = r\"C:\\Users\\thaim\\Videos\\AI_LEDS\\second_dataset\\video_test_output\"\n",
    "\n",
    "# Model and grid settings\n",
    "MODEL_INPUT_SIZE = 224  # matches training\n",
    "BOX_SIZE = 224          # grid box size (can change)\n",
    "OVERLAP_PERCENT = 0     # 0-90, controls stride; 0 means no overlap\n",
    "PROCESS_EVERY_N_FRAMES = 1  # process every Nth frame\n",
    "CONF_THRESHOLD = 0.5        # probability threshold for detector\n",
    "\n",
    "# Display and output\n",
    "DISPLAY_WINDOW = True\n",
    "SAVE_OUTPUT = True\n",
    "DRAW_GRID = True\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 0.4\n",
    "THICKNESS = 1\n",
    "\n",
    "# Color BGR tuples\n",
    "COLOR_POS = (0, 200, 0)\n",
    "COLOR_NEG = (0, 0, 200)\n",
    "COLOR_GRID = (180, 180, 180)\n",
    "\n",
    "# Fixed seed for any randomness (not used in deterministic grid)\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df647e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "        self.base.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DetectorClassifier().to(device)\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH, map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((MODEL_INPUT_SIZE, MODEL_INPUT_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def compute_stride(box_size, overlap_percent):\n",
    "    overlap_fraction = overlap_percent / 100.0\n",
    "    return max(1, int(box_size * (1 - overlap_fraction)))\n",
    "\n",
    "stride = compute_stride(BOX_SIZE, OVERLAP_PERCENT)\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f'Could not open video: {VIDEO_PATH}')\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "writer = None\n",
    "if SAVE_OUTPUT and OUTPUT_VIDEO_PATH:\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n",
    "\n",
    "frame_queue = queue.Queue(maxsize=1)\n",
    "last_overlay = None\n",
    "processed_frames = 0\n",
    "\n",
    "# Worker thread does inference so display loop stays smooth\n",
    "def worker():\n",
    "    global last_overlay, processed_frames\n",
    "    while True:\n",
    "        frame = frame_queue.get()\n",
    "        if frame is None:\n",
    "            break\n",
    "        overlay = frame.copy()\n",
    "        with torch.no_grad():\n",
    "            for y in range(0, height - BOX_SIZE + 1, stride):\n",
    "                for x in range(0, width - BOX_SIZE + 1, stride):\n",
    "                    crop = frame[y:y + BOX_SIZE, x:x + BOX_SIZE]\n",
    "                    process_now = (crop.shape[0] == BOX_SIZE and crop.shape[1] == BOX_SIZE)\n",
    "                    if not process_now:\n",
    "                        continue\n",
    "                    \n",
    "                    rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "                    pil_img = Image.fromarray(rgb)\n",
    "                    tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "                    prob = model(tensor).cpu().item()\n",
    "                    is_det = (prob >= CONF_THRESHOLD)\n",
    "                    \n",
    "                    if DRAW_GRID:\n",
    "                        color = COLOR_GRID\n",
    "                        cv2.rectangle(overlay, (x, y), (x + BOX_SIZE, y + BOX_SIZE), color, 1)\n",
    "                    \n",
    "                    color = COLOR_POS if is_det else COLOR_NEG\n",
    "                    if is_det or True:\n",
    "                        cv2.rectangle(overlay, (x, y), (x + BOX_SIZE, y + BOX_SIZE), color, 1)\n",
    "                        label = f\"{prob*100:.1f}%\"\n",
    "                        cv2.putText(overlay, label, (x + 4, y + 14), FONT, FONT_SCALE, color, THICKNESS, cv2.LINE_AA)\n",
    "        last_overlay = overlay\n",
    "        processed_frames += 1\n",
    "\n",
    "worker_thread = threading.Thread(target=worker, daemon=True)\n",
    "worker_thread.start()\n",
    "\n",
    "frame_idx = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        # Loop video instead of stopping\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        frame_idx = 0\n",
    "        continue\n",
    "\n",
    "    if frame_idx % PROCESS_EVERY_N_FRAMES == 0 and frame_queue.empty():\n",
    "        frame_queue.put(frame.copy())\n",
    "\n",
    "    overlay = last_overlay if last_overlay is not None else frame\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.write(overlay)\n",
    "    if DISPLAY_WINDOW:\n",
    "        cv2.imshow('Model Grid Predictions', overlay)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "frame_queue.put(None)\n",
    "worker_thread.join(timeout=2)\n",
    "cap.release()\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f'✓ PyTorch done: processed {processed_frames} inference frames')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
