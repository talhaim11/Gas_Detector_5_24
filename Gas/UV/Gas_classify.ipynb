{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52bd0d6b",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b7a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sn\n",
    "from sklearn import metrics\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, callbacks\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881f710",
   "metadata": {},
   "source": [
    "# Creating and trainning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ff019",
   "metadata": {},
   "source": [
    "## load and Preprocess data for model training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce79a67",
   "metadata": {},
   "source": [
    "### for 9 different gasses or states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\thaim\\OneDrive\\Desktop\\Tal_Projects\\Gas_detector\\UV\\Code\\code_files\\UV Spectrum\\Data train\\All_train_files\"\n",
    "LABELS = [ 'Ammonia','Benzene','H2S','Sulfur','Ozone','Toluene','Xylene','Regular','noise']  # Add more gas names as needed\n",
    "\n",
    "\n",
    "# Load and concatenate all CSV files into one DataFrame\n",
    "labels = []\n",
    "all_data = []\n",
    "inconsistent_data_count = 0  # Counter for inconsistent data entries\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Processing file: {filename}, shape: {df.shape}\")\n",
    "\n",
    "                # Drop completely blank rows and columns\n",
    "        df.dropna(how='all', axis=0, inplace=True)  # Drop blank rows\n",
    "        df.dropna(how='all', axis=1, inplace=True)  # Drop blank columns\n",
    "\n",
    "\n",
    "        # Determine the gas type based on the filename\n",
    "        gas_type = None\n",
    "        for i, gas in enumerate(LABELS):\n",
    "            if gas in filename:\n",
    "                gas_type = i\n",
    "                break\n",
    "\n",
    "        if gas_type is not None:\n",
    "            # Check if the file has one column or multiple columns\n",
    "            if df.shape[1] < 3:\n",
    "                # Single column file: add the entire column as one example\n",
    "                if len(df) == 311:\n",
    "                    all_data.append(df.iloc[:, 0].values.tolist())\n",
    "                    labels.append(gas_type)\n",
    "                else:\n",
    "                    print(f\"Skipping file {filename} due to incorrect length: {len(df)}\")\n",
    "            else:  # Multiple columns file\n",
    "                for row_idx, row in df.iterrows():\n",
    "                    if len(row) == 311:\n",
    "                        all_data.append(row.tolist())\n",
    "                        labels.append(gas_type)\n",
    "                        print(f\"Processed row {row_idx + 1} in multi-column file: {filename}\")\n",
    "                    else:\n",
    "                        print(f\"Skipping row {row_idx + 1} in {filename} due to incorrect length: {len(row)}\")\n",
    "                        inconsistent_data_count += 1\n",
    "\n",
    "# Check all data entries for consistent length\n",
    "if all(len(d) == 311 for d in all_data):\n",
    "    X = np.array(all_data)\n",
    "    y = np.array(labels)\n",
    "    print(\"Data consistency check passed.\")\n",
    "else:\n",
    "    print(\"Data consistency check failed. There are inconsistent entries.\")\n",
    "\n",
    "print(f\"Total inconsistent data entries: {inconsistent_data_count}\")\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Output shapes\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa402c0",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(250, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.2),  # Slightly lower dropout\n",
    "    layers.Dense(180, activation='tanh'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(100, activation='tanh'),\n",
    "    layers.Dense(50, activation='tanh'),\n",
    "    layers.Dropout(0.1),  # Slightly lower dropout\n",
    "    layers.Dense(20, activation='tanh'),\n",
    "    layers.Dense(len(LABELS), activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b71bf",
   "metadata": {},
   "source": [
    "## Model activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b057f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccbc69d",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca731de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d616e1",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b1534",
   "metadata": {},
   "source": [
    "## load weights and the Model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1b3c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = r\"C:\\Users\\thaim\\OneDrive\\Desktop\\Tal_Projects\\Gas_detector\\General_Codes\\Gas_detector\\model_weights\\model_tanh_250-9_dropout-02_LR-0001.weights.h5\"\n",
    "model.load_weights(weights_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf63690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential([\n",
    "    layers.Dense(250, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.2),  # Slightly lower dropout\n",
    "    layers.Dense(180, activation='tanh'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(100, activation='tanh'),\n",
    "    layers.Dense(50, activation='tanh'),\n",
    "    layers.Dropout(0.1),  # Slightly lower dropout\n",
    "    layers.Dense(20, activation='tanh'),\n",
    "    layers.Dense(len(LABELS))\n",
    "])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217bedf",
   "metadata": {},
   "source": [
    "## Reading Test Data for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8405a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the X_axis values from a CSV file and creating reference vector variable of the spectrum needed\n",
    "predict_test = pd.read_csv(r\"C:\\Users\\thaim\\OneDrive\\Desktop\\Tal_Projects\\Gas_detector\\General_Codes\\Gas_Detector_5_24\\Records\\TEST-mix_for_model.csv\", header=None)\n",
    "vector_predict_test = predict_test.iloc[0, :].values  # numpy array of values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f36e2",
   "metadata": {},
   "source": [
    "## Visualization of Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEST=(vector_predict_test)\n",
    "plt.figure(num=None, figsize=(12, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(TEST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d7a1c",
   "metadata": {},
   "source": [
    "## Performing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TEST=np.expand_dims(TEST, axis=0)\n",
    "print(\"Test Input Shape:\", TEST.shape)\n",
    "\n",
    "# Perform inference to get predicted probabilities\n",
    "probabilities = model2.predict(TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013ae1b",
   "metadata": {},
   "source": [
    "## Plotting the Probability Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07defc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting probabilities for a specific sample (e.g., the first sample in the test set)\n",
    "sample_index = 0\n",
    "sample_probabilities = probabilities[sample_index]\n",
    "\n",
    "# Plotting the probability distribution with numerical labels\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(LABELS, sample_probabilities)\n",
    "plt.xlabel('Gas Type')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Probability Distribution for Sample')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adding numerical labels to the bars\n",
    "for bar, prob in zip(bars, sample_probabilities):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height, f'{prob:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66ebec",
   "metadata": {},
   "source": [
    "## Batch Prediction & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aset = X_test\n",
    "Bset = y_test\n",
    "\n",
    "# Use a suitable batch size\n",
    "batch_size = 32  # Adjust as needed based on your system's memory capacity\n",
    "\n",
    "output = []\n",
    "for i in range(0, len(Aset), batch_size):\n",
    "    batch = Aset[i:i+batch_size]\n",
    "    predictions = model.predict(batch, verbose=0)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    output.extend(predicted_labels)\n",
    "\n",
    "output = np.array(output)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = metrics.confusion_matrix(Bset, output)\n",
    "df_cm = pd.DataFrame(cm, range(np.max(Bset) + 1), range(np.max(Bset) + 1))\n",
    "\n",
    "# Display the confusion matrix using seaborn\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.heatmap(df_cm, annot=True, fmt='g')  # Annotate cells with numbers\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f2eb4",
   "metadata": {},
   "source": [
    "## Testing with Excel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"C:\\Users\\thaim\\OneDrive\\Desktop\\Tal_Projects\\Gas cells check\\Amonia 6000\\05_03.xlsx\"\n",
    "\n",
    "xsls_files = pd.read_excel(path)\n",
    "t=np.array(xsls_files)\n",
    "test_input=(t[3,647:958]/100).astype('float32')\n",
    "#\n",
    "figure(num=None, figsize=(12, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(test_input)\n",
    "\n",
    "index=10\n",
    "#X_test[index,:]\n",
    "input1=np.expand_dims(test_input, axis=0)\n",
    "output=model.predict(input1)\n",
    "output\n",
    "\n",
    "\n",
    "LABELS[np.argmax(output)]\n",
    "LABELS[y_test[index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c9a4b",
   "metadata": {},
   "source": [
    "# full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# Folder path for CSV files\n",
    "folder_path = r\"C:\\Users\\thaim\\OneDrive\\Desktop\\Tal_Projects\\Gas_detector\\UV\\Code\\code_files\\UV Spectrum\\Data train\\All_train_files\"\n",
    "LABELS = ['Ammonia', 'Benzene', 'H2S', 'Sulfur', 'Ozone', 'Toluene', 'Xylene', 'Regular', 'noise']\n",
    "\n",
    "# Load and concatenate all CSV files into one DataFrame\n",
    "labels = []\n",
    "all_data = []\n",
    "inconsistent_data_count = 0\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Processing file: {filename}, shape: {df.shape}\")\n",
    "\n",
    "        # Drop completely blank rows and columns\n",
    "        df.dropna(how='all', axis=0, inplace=True)\n",
    "        df.dropna(how='all', axis=1, inplace=True)\n",
    "\n",
    "        # Determine the gas type based on the filename\n",
    "        gas_type = None\n",
    "        for i, gas in enumerate(LABELS):\n",
    "            if gas in filename:\n",
    "                gas_type = i\n",
    "                break\n",
    "\n",
    "        if gas_type is not None:\n",
    "            # Check if the file has one column or multiple columns\n",
    "            if df.shape[1] < 3:\n",
    "                # Single column file: add the entire column as one example\n",
    "                if len(df) == 311:\n",
    "                    all_data.append(df.iloc[:, 0].values.tolist())\n",
    "                    labels.append(gas_type)\n",
    "                else:\n",
    "                    print(f\"Skipping file {filename} due to incorrect length: {len(df)}\")\n",
    "            else:  # Multiple columns file\n",
    "                for row_idx, row in df.iterrows():\n",
    "                    if len(row) == 311:\n",
    "                        all_data.append(row.tolist())\n",
    "                        labels.append(gas_type)\n",
    "                        print(f\"Processed row {row_idx + 1} in multi-column file: {filename}\")\n",
    "                    else:\n",
    "                        print(f\"Skipping row {row_idx + 1} in {filename} due to incorrect length: {len(row)}\")\n",
    "                        inconsistent_data_count += 1\n",
    "\n",
    "# Check all data entries for consistent length\n",
    "if all(len(d) == 311 for d in all_data):\n",
    "    X = np.array(all_data)\n",
    "    y = np.array(labels)\n",
    "    print(\"Data consistency check passed.\")\n",
    "else:\n",
    "    print(\"Data consistency check failed. There are inconsistent entries.\")\n",
    "\n",
    "print(f\"Total inconsistent data entries: {inconsistent_data_count}\")\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Output shapes\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Neural Network Model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(250, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(180, activation='tanh'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(100, activation='tanh'),\n",
    "    layers.Dense(50, activation='tanh'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(20, activation='tanh'),\n",
    "    layers.Dense(len(LABELS), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Loss: {loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
