{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec7af35d",
   "metadata": {},
   "source": [
    "# Video Recording Cell - Add RAW video recording capability (CLEAN VERSION)\n",
    "# !/usr/bin/python377\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f0db0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Converting to INT8 TFLite...\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\thaim\\AppData\\Local\\Temp\\tmp2ivinmta\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\thaim\\AppData\\Local\\Temp\\tmp2ivinmta\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\thaim\\AppData\\Local\\Temp\\tmp2ivinmta'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2262406910352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406912080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406912272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406911888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406910928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406912656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406914000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406914192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406913808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406913040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406914576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406915920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406916112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406915728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406914960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406916496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406917840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406918032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406917648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406916880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406918416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406919760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406919952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406919568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406918800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406920336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406921680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406919184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406921488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406920720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262406921872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411642256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411642448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411642064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411641488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411642832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411644176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411644368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411643984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411643216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411644752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411646096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411646288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411645904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411645136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411646672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411648016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411648208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411647824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411647056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411648592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411649936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411650128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411649744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411648976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411650512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411651856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411652048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411651664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411650896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411652432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411653776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411653968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411653584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411652816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411654352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411655696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411655888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411655504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411654736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411656272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411854288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411854864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411641104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411656656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411854672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411856208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411856400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411856016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411855248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411856784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411858128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411858320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411857936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411857168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411858704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411860048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411860240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411859856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411859088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411860624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411861968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411862160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411861776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411861008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411862544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411863888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411864080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411863696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411862928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411864464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411865808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411866000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411865616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411864848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411866384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411867728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411867920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411867536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411866768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411868304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411869648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411867152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411869456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411868688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262411869840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412035088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412034320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412035280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412034512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412036048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412037392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412037584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412037200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412036432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412037968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412039312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412039504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412039120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412038352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412039888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412041232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412041424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412041040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412040272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412041808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412043152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412043344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412042960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412042192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412043728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412045072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412045264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412044880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412044112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412045648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412046992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412047184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412046800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412046032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412047568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412048912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412049104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412048720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412047952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412049488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412165584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412166160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412035664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412049872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412165968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412167504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412167696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412167312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412166544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412168080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412169424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412169616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412169232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412168464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412170000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412171344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412171536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412171152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412170384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412171920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412173264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412173456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412173072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412172304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412173840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412175184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412175376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412174992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412174224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412175760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412177104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412177296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412176912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412176144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412177680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412179024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412179216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412178832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412178064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412179600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412180944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412178448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412180752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412179984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412181136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412363152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412363344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412362960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412362384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412363728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412365072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412365264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412364880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412364112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412365648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412366992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412367184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412366800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412366032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412367568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412368912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412369104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412368720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412367952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412369488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412370832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412371024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412370640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412369872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412371408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412372752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412372944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412372560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412371792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412373328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412374672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412374864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412374480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412373712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412375248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412376592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412376784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412376400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412375632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412377168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412558608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412559376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412362000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412377552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412559184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412560720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412560912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412560528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412559760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412561296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412562640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412562832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412562448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412561680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412563216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412564560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412564752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412564368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412563600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412565136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412566480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412565520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2262412567824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thaim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\thaim\\Downloads\\detector_classifier_mobilenetv2_int8.tflite (2.74 MB)\n",
      "Transfer to RPi: scp detector_classifier_mobilenetv2_int8.tflite reucam@169.254.104.28:/home/reucam/dev/fgvideo/src/MODEL_WEIGHTS/\n"
     ]
    }
   ],
   "source": [
    "# ===== CONVERT .H5 MODEL TO INT8 QUANTIZED .TFLITE (RUN ON WINDOWS) =====\n",
    "# INT8 quantization uses ~4x less memory than float32 - critical for low-RAM RPi\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "h5_path = r\"C:\\Users\\thaim\\Downloads\\detector_classifier_mobilenetv2_final.h5\"\n",
    "tflite_path = r\"C:\\Users\\thaim\\Downloads\\detector_classifier_mobilenetv2_int8.tflite\"\n",
    "\n",
    "model = keras.models.load_model(h5_path)\n",
    "print(\"Model loaded. Converting to INT8 TFLite...\")\n",
    "\n",
    "# Representative dataset generator for INT8 calibration (uses random images)\n",
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "        data = np.random.rand(1, 224, 224, 3).astype(np.float32) * 255.0  # Random RGB images\n",
    "        yield [data]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8   # Input: uint8 (0-255)\n",
    "converter.inference_output_type = tf.float32  # Output: float32 (for sigmoid probability)\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"Saved: {tflite_path} ({len(tflite_model) / 1024 / 1024:.2f} MB)\")\n",
    "print(\"Transfer to RPi: scp detector_classifier_mobilenetv2_int8.tflite reucam@169.254.104.28:/home/reucam/dev/fgvideo/src/MODEL_WEIGHTS/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TENSORFLOW INSTALLATION COMMANDS FOR RASPBERRY PI =====\n",
    "# Run these commands in the Raspberry Pi terminal (via MobaXterm SSH)\n",
    "# Do NOT run this cell - these are terminal commands only\n",
    "# RPi IP: 169.254.104.28 (link-local - no internet, use offline install)\n",
    "\n",
    "\"\"\"\n",
    "========== OFFLINE INSTALLATION (RPi has no internet) ==========\n",
    "\n",
    "STEP 1: On your Windows PC (with internet), open cmd and run:\n",
    "    pip download tensorflow -d C:\\tf_wheels\n",
    "\n",
    "STEP 2: Transfer to RPi (from Windows cmd or MobaXterm local terminal):\n",
    "    scp -r C:\\tf_wheels reucam@169.254.104.28:/home/reucam/\n",
    "\n",
    "STEP 3: On RPi (via SSH), install from local files:\n",
    "    pip3 install /home/reucam/tf_wheels/*.whl\n",
    "\n",
    "STEP 4: Verify installation on RPi:\n",
    "    python3 -c \"import tensorflow as tf; print('TensorFlow:', tf.__version__)\"\n",
    "\n",
    "STEP 5: Create model directory on RPi:\n",
    "    mkdir -p /home/reucam/dev/fgvideo/src/MODEL_WEIGHTS\n",
    "\n",
    "STEP 6: Copy your model file (from Windows):\n",
    "    scp detector_classifier_mobilenetv2_final.h5 reucam@169.254.104.28:/home/reucam/dev/fgvideo/src/MODEL_WEIGHTS/\n",
    "\n",
    "========== ALTERNATIVE: TensorFlow Lite (lighter, faster) ==========\n",
    "# If full TensorFlow is too heavy for RPi, use TFLite instead:\n",
    "# On Windows: pip download tflite-runtime -d C:\\tf_wheels\n",
    "# Then transfer and install same way as above\n",
    "\n",
    "========== IF YOU NEED DEPENDENCIES ==========\n",
    "# These may already be installed. If pip install fails, try on RPi:\n",
    "    sudo apt-get install -y python3-pip python3-dev\n",
    "    sudo apt-get install -y libhdf5-dev libhdf5-103\n",
    "    sudo apt-get install -y libatlas-base-dev\n",
    "\"\"\"\n",
    "print(\"See comments above for offline installation commands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d84472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LED DETECTOR WITH AI MODEL VERIFICATION (TFLite) =====\n",
    "# Mode 1: Full FFT scan | Mode 2: Refine bbox | Mode 3: Model prediction | Mode 4: Focused FFT + validation\n",
    "import struct\n",
    "import sys\n",
    "import threading\n",
    "import logging\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import configparser\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import signal\n",
    "import atexit\n",
    "import fcntl\n",
    "\n",
    "# TFLite imports (for armv7l RPi)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "TFLITE_AVAILABLE = False\n",
    "tflite_interpreter = None\n",
    "try:\n",
    "    import tflite_runtime.interpreter as tflite\n",
    "    TFLITE_AVAILABLE = True\n",
    "    logging.info(\"TFLite runtime loaded successfully\")\n",
    "except ImportError:\n",
    "    logging.warning(\"TFLite not available - Mode 3 will auto-confirm all detections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2261bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION AND INITIALIZATION =====\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "cv2.setNumThreads(1)\n",
    "signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n",
    "\n",
    "sys.path.append(\"/var/fgvideo/src\")\n",
    "sys.path.append(\"/var/fgvideo/src/dist-packages\")\n",
    "\n",
    "from ReuCameraPy.ReuCameraPy import *\n",
    "from ReuOnvifPy import config\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname).1s] %(message)s\")\n",
    "\n",
    "FRAME_W, FRAME_H = config.get_snapshot_resolution()\n",
    "\n",
    "# Load JSON config\n",
    "config_path = os.path.join(os.path.dirname(__file__), 'camera_alignment_config.json')\n",
    "try:\n",
    "    with open(config_path, 'r') as f:\n",
    "        cfg = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"Configuration file not found at {config_path}\")\n",
    "    sys.exit(1)\n",
    "except json.JSONDecodeError:\n",
    "    logging.error(f\"Error decoding JSON from {config_path}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ===== CONFIGURABLE PARAMETERS (CHANGE THESE AS NEEDED) =====\n",
    "MARGIN_MODE4 = 40                    # Margin for Mode 4 ROI (bigger box for focused scanning)\n",
    "MODEL_VALIDATION_INTERVAL = 10       # Re-run model every N frames in Mode 4\n",
    "MODEL_CONFIDENCE_THRESHOLD = 0.5     # Threshold for positive prediction (0.5 = 50%)\n",
    "FALSE_ALARM_DISPLAY_TIME = 4.0       # Seconds to display FALSE ALARM\n",
    "MODEL_INPUT_SIZE = 224               # Model input size (224x224)\n",
    "\n",
    "# ===== LOAD TFLITE MODEL (INT8 QUANTIZED) =====\n",
    "MODEL_PATH = \"/home/reucam/dev/fgvideo/src/MODEL_WEIGHTS/detector_classifier_mobilenetv2_int8.tflite\"\n",
    "tflite_interpreter = None\n",
    "input_details = None\n",
    "output_details = None\n",
    "if TFLITE_AVAILABLE:\n",
    "    try:\n",
    "        tflite_interpreter = tflite.Interpreter(model_path=MODEL_PATH)\n",
    "        tflite_interpreter.allocate_tensors()\n",
    "        input_details = tflite_interpreter.get_input_details()\n",
    "        output_details = tflite_interpreter.get_output_details()\n",
    "        logging.info(f\"TFLite INT8 model loaded: {MODEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load TFLite model: {e}\")\n",
    "        tflite_interpreter = None\n",
    "\n",
    "# ===== SENSOR/CAMERA PARAMETERS =====\n",
    "REUCAM_NOTIFY_FIREBORDER = 0x8010\n",
    "sensor_width = cfg[\"sensor_width\"]\n",
    "sensor_height = cfg[\"sensor_height\"]\n",
    "pixel_size = cfg[\"pixel_size\"]\n",
    "focal_length = cfg[\"focal_length\"]\n",
    "BOX_SIZE = cfg[\"BOX_SIZE\"]\n",
    "LED_FREQ = cfg[\"LED_FREQ\"]\n",
    "FREQ_TOL = cfg[\"FREQ_TOL\"]\n",
    "FREQ_RANGE = 2\n",
    "CALC_LENGTH = cfg[\"CALC_LENGTH\"]\n",
    "MARGIN = cfg[\"MARGIN\"]\n",
    "fourier_peak_threshold = cfg[\"fourier_peak_threshold\"]\n",
    "\n",
    "frame_width = FRAME_W\n",
    "frame_height = FRAME_H\n",
    "\n",
    "pixel_size_eff_x = pixel_size * sensor_width / frame_width\n",
    "pixel_size_eff_y = pixel_size * sensor_height / frame_height\n",
    "\n",
    "IFOV_x = 2 * np.degrees(np.arctan(pixel_size_eff_x / (2 * focal_length)))\n",
    "IFOV_y = 2 * np.degrees(np.arctan(pixel_size_eff_y / (2 * focal_length)))\n",
    "\n",
    "# ===== Global buffers =====\n",
    "WI, HE = frame_width // BOX_SIZE + 1, frame_height // BOX_SIZE + 1\n",
    "fourier_buff = np.zeros((CALC_LENGTH, HE, WI))\n",
    "freq_counter = np.zeros((HE, WI))\n",
    "pixel_count_buff = np.zeros(CALC_LENGTH)\n",
    "\n",
    "# ===== Global variables =====\n",
    "mode = 1\n",
    "roi_mode1 = None\n",
    "MARGIN3_factor = 1\n",
    "LED_CIRCLE_SIZE = 0.05\n",
    "bbox_mode2 = None\n",
    "flicker_buff = None\n",
    "scounter = tcounter = m2counter = 0\n",
    "top_point, bottom_point = None, None\n",
    "pixel_buffer = None\n",
    "frame_idx = 0\n",
    "flicker_stable_count = 0\n",
    "STABLE_FRAMES_THRESHOLD = 5\n",
    "bbox_mode3 = None\n",
    "ROI_STABLE_FRAMES = 1\n",
    "ROI_HISTORY = 10\n",
    "roi_history = []\n",
    "failure_count_m3 = 0\n",
    "MAX_FAILURES_M3 = 40\n",
    "roi_stable_count = 0\n",
    "\n",
    "# ===== MODE 4 VARIABLES =====\n",
    "bbox_mode4 = None                    # ROI for focused scanning in Mode 4\n",
    "mode4_frame_counter = 0              # Counter for model re-validation\n",
    "mode4_fft_failure_count = 0          # Counter for FFT frequency failures\n",
    "MAX_FFT_FAILURES_MODE4 = 20          # Max failures before returning to Mode 1\n",
    "last_prediction_result = None        # Store last prediction (True/False)\n",
    "false_alarm_start_time = None        # Timer for FALSE ALARM display\n",
    "\n",
    "# ===== VIDEO RECORDING VARIABLES =====\n",
    "is_recording = False\n",
    "video_writer = None\n",
    "recording_filename = \"\"\n",
    "frame_count_record = 0\n",
    "recording_start_time = None\n",
    "\n",
    "# ===== Read FPS from /etc/reucamd.conf =====\n",
    "fps_conf = configparser.ConfigParser()\n",
    "fps_conf.read(\"/etc/reucamd.conf\")\n",
    "FPS = fps_conf.getint(\"camera\", \"framerate\", fallback=cfg[\"FPS\"])\n",
    "\n",
    "cross_conf_path = os.path.join(os.path.dirname(__file__), '..', 'etc', 'reucamd.conf')\n",
    "PIXEL_HISTORY = FPS * 2\n",
    "\n",
    "# ===== AI MODEL PREDICTION FUNCTION (TFLite INT8) =====\n",
    "def predict_detector(frame_rgb, bbox):\n",
    "    \"\"\"Run TFLite INT8 model on ROI. Returns (is_detected, confidence)\"\"\"\n",
    "    if tflite_interpreter is None:\n",
    "        return True, 1.0  # Auto-confirm if no model\n",
    "    \n",
    "    x, y, w, h = [int(v) for v in bbox]\n",
    "    x, y = max(0, x), max(0, y)\n",
    "    x2, y2 = min(frame_rgb.shape[1], x + w), min(frame_rgb.shape[0], y + h)\n",
    "    \n",
    "    if x2 <= x or y2 <= y:\n",
    "        return False, 0.0\n",
    "    \n",
    "    roi = frame_rgb[y:y2, x:x2]\n",
    "    roi_resized = cv2.resize(roi, (MODEL_INPUT_SIZE, MODEL_INPUT_SIZE))\n",
    "    roi_input = np.expand_dims(roi_resized, axis=0).astype(np.uint8)  # INT8 model expects uint8\n",
    "    \n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], roi_input)\n",
    "    tflite_interpreter.invoke()\n",
    "    prediction = tflite_interpreter.get_tensor(output_details[0]['index'])[0][0]\n",
    "    \n",
    "    is_detected = prediction > MODEL_CONFIDENCE_THRESHOLD\n",
    "    return is_detected, float(prediction)\n",
    "\n",
    "# ===== VIDEO RECORDING FUNCTIONS =====\n",
    "def start_recording():\n",
    "    global is_recording, video_writer, recording_filename, frame_count_record, recording_start_time\n",
    "    if is_recording:\n",
    "        logging.info(\"Already recording!\")\n",
    "        return\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    recording_filename = f\"RAW_camera_recording_{timestamp}.mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(recording_filename, fourcc, FPS, (FRAME_W, FRAME_H))\n",
    "    if not video_writer.isOpened():\n",
    "        logging.error(\"Failed to initialize RAW video writer\")\n",
    "        return\n",
    "    is_recording = True\n",
    "    recording_start_time = time.time()\n",
    "    frame_count_record = 0\n",
    "    logging.info(f\"RAW Recording started: {recording_filename}\")\n",
    "    print(f\"RAW_RECORDING_STARTED: {recording_filename}\")\n",
    "\n",
    "def stop_recording():\n",
    "    global is_recording, video_writer, recording_filename, frame_count_record\n",
    "    if not is_recording:\n",
    "        logging.info(\"Not recording!\")\n",
    "        return\n",
    "    if video_writer:\n",
    "        video_writer.release()\n",
    "        video_writer = None\n",
    "    recording_duration = time.time() - recording_start_time if recording_start_time else 0\n",
    "    is_recording = False\n",
    "    logging.info(f\"RAW Recording stopped. Saved: {recording_filename} ({frame_count_record} frames, {recording_duration:.1f}s)\")\n",
    "    print(f\"RAW_RECORDING_STOPPED: {recording_filename}, frames: {frame_count_record}, duration: {recording_duration:.1f}s\")\n",
    "\n",
    "def record_raw_frame(yuv_data):\n",
    "    global frame_count_record\n",
    "    if is_recording and video_writer and yuv_data is not None:\n",
    "        raw_frame = convert_yuv_to_bgr(yuv_data, FRAME_W, FRAME_H)\n",
    "        video_writer.write(raw_frame)\n",
    "        frame_count_record += 1\n",
    "\n",
    "# ===== REUCAM SERVICE =====\n",
    "class ReucamClient(ReuCameraClient):\n",
    "    def __init__(self, svc):\n",
    "        super().__init__()\n",
    "        self.svc = svc\n",
    "\n",
    "    def cmd_onFrame(self, cmd):\n",
    "        frame = cmd.frame\n",
    "        self.svc.onFrame(frame)\n",
    "\n",
    "class ReucamService(threading.Thread):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.daemon = True\n",
    "        self.client = None\n",
    "        self.lock = threading.Lock()\n",
    "        self.I = None\n",
    "        self.ts = None\n",
    "        self.frame_event = threading.Event()\n",
    "        self.bActive = False\n",
    "\n",
    "    def run(self):\n",
    "        self.bActive = True\n",
    "        logging.info(\"ReucamService: started\")\n",
    "        backoff = 1.0\n",
    "        while self.bActive:\n",
    "            client = None\n",
    "            try:\n",
    "                client = ReucamClient(self)\n",
    "                self.client = client\n",
    "                client.connect(ReuCameraService.SERVER_ADDR)\n",
    "                logging.info(\"ReucamService: connected\")\n",
    "                client.cmd_StartStream(3)\n",
    "                client.recv_loop()\n",
    "            except Exception:\n",
    "                logging.exception(\"ReucamService error - reconnecting\")\n",
    "                try:\n",
    "                    if client:\n",
    "                        client.close()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                self.client = None\n",
    "                time.sleep(backoff)\n",
    "                backoff = min(backoff * 2, 30.0)\n",
    "                continue\n",
    "            else:\n",
    "                backoff = 1.0\n",
    "                try:\n",
    "                    if client:\n",
    "                        client.close()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                self.client = None\n",
    "            finally:\n",
    "                time.sleep(0.2)\n",
    "\n",
    "    def onFrame(self, frame):\n",
    "        try:\n",
    "            if not frame:\n",
    "                return\n",
    "            if getattr(frame, 'is_raw', False) and len(frame.data) == FRAME_W * FRAME_H * 3 // 2:\n",
    "                with self.lock:\n",
    "                    self.I = frame.data\n",
    "                    self.ts = frame.timestamp\n",
    "                    self.frame_event.set()\n",
    "        except Exception:\n",
    "            logging.exception(\"Error in onFrame handler\")\n",
    "\n",
    "    def onFireBorder(self, param1, param2):\n",
    "        try:\n",
    "            if self.client is None:\n",
    "                return\n",
    "            self.client.cmd_Notify(REUCAM_NOTIFY_FIREBORDER, param1, param2)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# ===== AUXILIARY FUNCTIONS =====\n",
    "def convert_yuv_to_bgr(yuv_data, width, height):\n",
    "    y_size = width * height\n",
    "    uv_size = y_size // 4\n",
    "    y = np.frombuffer(yuv_data[0:y_size], dtype=np.uint8).reshape((height, width))\n",
    "    u = np.frombuffer(yuv_data[y_size:y_size+uv_size], dtype=np.uint8).reshape((height//2, width//2))\n",
    "    v = np.frombuffer(yuv_data[y_size+uv_size:y_size+2*uv_size], dtype=np.uint8).reshape((height//2, width//2))\n",
    "    u_up = cv2.resize(u, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "    v_up = cv2.resize(v, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "    yuv = cv2.merge((y, u_up, v_up))\n",
    "    return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "def get_fft_frequency(sig, fps):\n",
    "    sig = sig - np.mean(sig)\n",
    "    fft_vals = np.abs(np.fft.rfft(sig))[0:]\n",
    "    f_axis = np.linspace(0, fps / 2, len(fft_vals) + 1)[1:]\n",
    "    if len(fft_vals) == 0:\n",
    "        return 0\n",
    "    f_min, f_max = LED_FREQ - FREQ_TOL, LED_FREQ + FREQ_TOL\n",
    "    f_min_R, f_max_R = LED_FREQ - FREQ_RANGE, LED_FREQ + FREQ_RANGE\n",
    "    mask_freq = (f_axis >= f_min) & (f_axis <= f_max)\n",
    "    mask_range = (f_axis >= f_min_R) & (f_axis <= f_max_R)\n",
    "    mask_outer_only = mask_range & (~mask_freq)\n",
    "    if len(fft_vals[mask_freq]) == 0 or len(fft_vals[mask_outer_only]) == 0:\n",
    "        return 0\n",
    "    inner_sum = np.sum(fft_vals[mask_freq]) / len(fft_vals[mask_freq])\n",
    "    outer_only_sum = np.sum(fft_vals[mask_outer_only]) / len(fft_vals[mask_outer_only])\n",
    "    return LED_FREQ if 0.2 * inner_sum > outer_only_sum else 0\n",
    "\n",
    "def process_box(frame, gray, B2, h, w):\n",
    "    b2 = B2[h*BOX_SIZE:(h+1)*BOX_SIZE, w*BOX_SIZE:(w+1)*BOX_SIZE]\n",
    "    z = np.roll(fourier_buff[:, h, w], 1)\n",
    "    z[0] = np.sum(b2)\n",
    "    fourier_buff[:, h, w] = z\n",
    "    freq = get_fft_frequency(fourier_buff[:, h, w], FPS)\n",
    "    if abs(freq - LED_FREQ) < FREQ_TOL:\n",
    "        freq_counter[h, w] += 1\n",
    "        x1, y1 = w * BOX_SIZE, h * BOX_SIZE\n",
    "        x2 = min((w + 1) * BOX_SIZE, frame_width)\n",
    "        y2 = min((h + 1) * BOX_SIZE, frame_height)\n",
    "        return (x1, y1, x2, y2), freq\n",
    "    freq_counter[h, w] = 0\n",
    "    return None, None\n",
    "\n",
    "def process_flicker_pixels(frame, roi, flicker_buff, frame_count, buffer_len=10, thresh=60):\n",
    "    global flicker_stable_count, mode, bbox_mode3\n",
    "    if roi is None:\n",
    "        return None, flicker_buff, frame_count, bbox_mode3\n",
    "    x1, y1, x2, y2 = roi\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(frame.shape[1]-1, x2), min(frame.shape[0]-1, y2)\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None, flicker_buff, frame_count, bbox_mode3\n",
    "    roi_mode2 = frame[y1:y2, x1:x2]\n",
    "    roi_gray = cv2.cvtColor(roi_mode2, cv2.COLOR_BGR2GRAY)\n",
    "    h, w = roi_gray.shape\n",
    "    if flicker_buff is None or flicker_buff.shape[1:] != (h, w):\n",
    "        flicker_buff = np.zeros((buffer_len, h, w), dtype=np.uint8)\n",
    "        frame_count = 0\n",
    "        flicker_stable_count = 0\n",
    "    flicker_buff = np.roll(flicker_buff, -1, axis=0)\n",
    "    flicker_buff[-1] = roi_gray\n",
    "    frame_count += 1\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    if frame_count > buffer_len:\n",
    "        diff = flicker_buff.max(axis=0) - flicker_buff.min(axis=0)\n",
    "        mask[diff > thresh] = 200\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if contours:\n",
    "            all_points = np.vstack(contours)\n",
    "            x, y, w_box, h_box = cv2.boundingRect(all_points)\n",
    "            MARGIN3 = 25\n",
    "            x_margin = max(0, x1 + x - MARGIN3)\n",
    "            y_margin = max(0, y1 + y - MARGIN3)\n",
    "            x2_margin = min(frame.shape[1], x1 + x + w_box + MARGIN3)\n",
    "            y2_margin = min(frame.shape[0], y1 + y + h_box + MARGIN3)\n",
    "            cv2.rectangle(frame, (x1+x, y1+y), (x1+x+w_box, y1+y+h_box), (0, 0, 255), 2)\n",
    "            flicker_stable_count += 1\n",
    "            if flicker_stable_count >= STABLE_FRAMES_THRESHOLD:\n",
    "                bbox_mode3 = (x_margin, y_margin, x2_margin - x_margin, y2_margin - y_margin)\n",
    "                logging.info(f\"bbox_mode3={bbox_mode3}\")\n",
    "                mode = 3\n",
    "        else:\n",
    "            flicker_stable_count = 0\n",
    "            bbox_mode3 = None\n",
    "            mode = 1\n",
    "    return mask, flicker_buff, frame_count, bbox_mode3\n",
    "\n",
    "def convert_bboxes_to_reucam_params(bbox):\n",
    "    x0 = math.ceil(bbox[0] * FRAME_W / frame_width)\n",
    "    x1 = math.ceil(bbox[2] * FRAME_W / frame_width)\n",
    "    y0 = math.ceil(bbox[1] * FRAME_H / frame_height)\n",
    "    y1 = math.ceil(bbox[3] * FRAME_H / frame_height)\n",
    "    w, h = x1 - x0, y1 - y0\n",
    "    if frame_width > 640:\n",
    "        w, h = math.ceil(w / 2), math.ceil(h / 2)\n",
    "    x0 = max(0, min(65535, x0))\n",
    "    y0 = max(0, min(65535, y0))\n",
    "    w, h = max(0, min(255, w)), max(0, min(255, h))\n",
    "    param1 = x0\n",
    "    try:\n",
    "        param2 = struct.pack('HBB', y0, w, h)\n",
    "    except struct.error:\n",
    "        param2 = struct.pack('HBB', 0, 0, 0)\n",
    "    param2 = struct.unpack('i', param2)[0]\n",
    "    return param1, param2\n",
    "\n",
    "def calculate_alignment(bbox, center_x0, center_y0):\n",
    "    \"\"\"Calculate alignment angles from bbox center to frame center\"\"\"\n",
    "    x, y, w, h = [int(v) for v in bbox]\n",
    "    center_x = x + w // 2\n",
    "    center_y = y + h // 2\n",
    "    ANG_THRESH = 0.2\n",
    "    y_alignment_deg = -2 * np.round((center_x - center_x0) * IFOV_x, 1)\n",
    "    y_alignment_deg = 0 if abs(y_alignment_deg) < ANG_THRESH else y_alignment_deg\n",
    "    x_alignment_deg = -2 * np.round((center_y - center_y0) * IFOV_y, 1)\n",
    "    x_alignment_deg = 0 if abs(x_alignment_deg) < ANG_THRESH else x_alignment_deg\n",
    "    return x_alignment_deg, y_alignment_deg, center_x, center_y\n",
    "\n",
    "def reset_to_mode1():\n",
    "    \"\"\"Reset all state variables and return to Mode 1\"\"\"\n",
    "    global mode, bbox_mode3, bbox_mode4, flicker_stable_count, roi_stable_count\n",
    "    global fourier_buff, flicker_buff, mode4_frame_counter, mode4_fft_failure_count\n",
    "    global failure_count_m3, tcounter, pixel_count_buff, last_prediction_result\n",
    "    mode = 1\n",
    "    bbox_mode3 = None\n",
    "    bbox_mode4 = None\n",
    "    flicker_stable_count = 0\n",
    "    roi_stable_count = 0\n",
    "    failure_count_m3 = 0\n",
    "    tcounter = 0\n",
    "    mode4_frame_counter = 0\n",
    "    mode4_fft_failure_count = 0\n",
    "    last_prediction_result = None\n",
    "    pixel_count_buff[:] = 0\n",
    "    fourier_buff = np.zeros((CALC_LENGTH, HE, WI))\n",
    "    flicker_buff = None\n",
    "    logging.info(\"Reset to Mode 1\")\n",
    "\n",
    "# Set stdin to non-blocking\n",
    "fd = sys.stdin.fileno()\n",
    "fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n",
    "fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n",
    "\n",
    "def check_stdin():\n",
    "    try:\n",
    "        line = sys.stdin.readline()\n",
    "        if line:\n",
    "            return line.strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "last_fireborder = None\n",
    "last_fire_time = 0\n",
    "\n",
    "# ===== MAIN LOOP =====\n",
    "if __name__ == \"__main__\":\n",
    "    reucam = ReucamService()\n",
    "    reucam.start()\n",
    "    \n",
    "    def cleanup():\n",
    "        try:\n",
    "            if is_recording:\n",
    "                stop_recording()\n",
    "            reucam.onFireBorder(0, 0)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    atexit.register(cleanup)\n",
    "    \n",
    "    def handle_signal(signum, frame):\n",
    "        cleanup()\n",
    "        sys.exit(0)\n",
    "    \n",
    "    signal.signal(signal.SIGINT, handle_signal)\n",
    "    signal.signal(signal.SIGTERM, handle_signal)\n",
    "    signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n",
    "\n",
    "    I = None\n",
    "    frame_count = 0\n",
    "    fps_start_time = time.time()\n",
    "    current_fps = FPS\n",
    "    \n",
    "    while True:\n",
    "        if reucam.frame_event.wait(1):\n",
    "            break\n",
    "        logging.info(\"Waiting for camera to connect...\")\n",
    "\n",
    "    print(\"=== LED DETECTOR WITH AI MODEL (TFLite INT8) ===\")\n",
    "    print(\"Commands: 's'=start recording, 'p'=stop recording\")\n",
    "    print(\"=================================================\")\n",
    "    \n",
    "    while True:\n",
    "        line = check_stdin()\n",
    "        if line == \"s\":\n",
    "            start_recording()\n",
    "        elif line == \"p\":\n",
    "            stop_recording()\n",
    "        elif line == \"RESET_BORDER\":\n",
    "            logging.info(\"Received RESET_BORDER command\")\n",
    "            reucam.onFireBorder(0, 0)\n",
    "            reset_to_mode1()\n",
    "\n",
    "        reucam.frame_event.clear()\n",
    "        if reucam.I == I:\n",
    "            continue\n",
    "        I = reucam.I\n",
    "        ts = reucam.ts\n",
    "\n",
    "        # FPS calculation\n",
    "        frame_count += 1\n",
    "        current_time = time.time()\n",
    "        if current_time - fps_start_time >= 1.0:\n",
    "            current_fps = frame_count / (current_time - fps_start_time)\n",
    "            print(f\"FPS: {current_fps:.1f} | Mode: {mode}\", flush=True)\n",
    "            frame_count = 0\n",
    "            fps_start_time = current_time\n",
    "            FPS = current_fps\n",
    "\n",
    "        if I is None:\n",
    "            continue\n",
    "\n",
    "        yuv_data = reucam.I\n",
    "        record_raw_frame(yuv_data)\n",
    "\n",
    "        # Extract grayscale and create display frame\n",
    "        y_size = FRAME_W * FRAME_H\n",
    "        gray = np.frombuffer(yuv_data[0:y_size], dtype=np.uint8).reshape((FRAME_H, FRAME_W))\n",
    "        _, B2 = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "        frame = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "        frame[10:20, 260:380, :] = 0  # Delete clock overlay\n",
    "\n",
    "        # Read cross position for alignment\n",
    "        cross_conf = configparser.ConfigParser()\n",
    "        cross_conf.read(cross_conf_path)\n",
    "        center_x0 = cross_conf.getint(\"camera\", \"cross_w_pos\", fallback=frame_width // 2)\n",
    "        center_y0 = cross_conf.getint(\"camera\", \"cross_h_pos\", fallback=frame_height // 2)\n",
    "\n",
    "        # ===== MODE 1: Full FFT Scan =====\n",
    "        if mode == 1:\n",
    "            print(\"1 0 0\", flush=True)\n",
    "            found_boxes = []\n",
    "            for h in range(HE):\n",
    "                for w in range(WI):\n",
    "                    cv2.rectangle(frame, (w*BOX_SIZE, h*BOX_SIZE), \n",
    "                                  (w*BOX_SIZE+BOX_SIZE, h*BOX_SIZE+BOX_SIZE), (255, 255, 255), 1)\n",
    "                    box, freq = process_box(frame, gray, B2, h, w)\n",
    "                    if box is not None:\n",
    "                        found_boxes.append(box)\n",
    "\n",
    "            if found_boxes:\n",
    "                x1 = max(0, min(b[0] for b in found_boxes) - MARGIN)\n",
    "                y1 = max(0, min(b[1] for b in found_boxes) - MARGIN)\n",
    "                x2 = min(frame_width, max(b[2] for b in found_boxes) + MARGIN)\n",
    "                y2 = min(frame_height, max(b[3] for b in found_boxes) + MARGIN)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "                roi_stable_count += 1\n",
    "                if roi_stable_count >= ROI_STABLE_FRAMES:\n",
    "                    bbox_mode2 = (x1 - MARGIN, y1 - MARGIN, x2 + MARGIN, y2 + MARGIN)\n",
    "                    mode = 2\n",
    "                    logging.info(\"Mode 1 -> Mode 2\")\n",
    "            else:\n",
    "                roi_stable_count = 0\n",
    "\n",
    "        # ===== MODE 2: Refine Bbox with Flicker Analysis =====\n",
    "        elif mode == 2:\n",
    "            frame = convert_yuv_to_bgr(yuv_data, FRAME_W, FRAME_H)\n",
    "            frame[10:20, 260:380, :] = 0\n",
    "            print(\"1 0 0\", flush=True)\n",
    "            if bbox_mode2 is not None:\n",
    "                mask, flicker_buff, frame_idx, bbox_mode3 = process_flicker_pixels(\n",
    "                    frame, bbox_mode2, flicker_buff, frame_idx)\n",
    "\n",
    "        # ===== MODE 3: AI Model Prediction =====\n",
    "        elif mode == 3:\n",
    "            frame = convert_yuv_to_bgr(yuv_data, FRAME_W, FRAME_H)\n",
    "            frame[10:20, 260:380, :] = 0\n",
    "            \n",
    "            if bbox_mode3 is not None:\n",
    "                x, y, w, h = [int(v) for v in bbox_mode3]\n",
    "                \n",
    "                # Run AI model prediction\n",
    "                is_detected, confidence = predict_detector(frame, bbox_mode3)\n",
    "                logging.info(f\"Model prediction: detected={is_detected}, confidence={confidence:.3f}\")\n",
    "                \n",
    "                if is_detected:\n",
    "                    # POSITIVE: Transition to Mode 4\n",
    "                    bbox_mode4 = (max(0, x - MARGIN_MODE4), \n",
    "                                  max(0, y - MARGIN_MODE4),\n",
    "                                  min(w + 2*MARGIN_MODE4, frame_width - x + MARGIN_MODE4),\n",
    "                                  min(h + 2*MARGIN_MODE4, frame_height - y + MARGIN_MODE4))\n",
    "                    mode4_frame_counter = 0\n",
    "                    mode4_fft_failure_count = 0\n",
    "                    last_prediction_result = True\n",
    "                    mode = 4\n",
    "                    logging.info(f\"Mode 3 -> Mode 4 (CONFIRMED) bbox_mode4={bbox_mode4}\")\n",
    "                    \n",
    "                    # Calculate and output alignment\n",
    "                    x_deg, y_deg, cx, cy = calculate_alignment(bbox_mode3, center_x0, center_y0)\n",
    "                    print(f\"0 {x_deg} {y_deg}\", flush=True)\n",
    "                    \n",
    "                    # Display CONFIRMED\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "                    cv2.putText(frame, \"CONFIRMED!\", (x, y - 10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3)\n",
    "                else:\n",
    "                    # NEGATIVE: FALSE ALARM\n",
    "                    false_alarm_start_time = time.time()\n",
    "                    last_prediction_result = False\n",
    "                    logging.info(\"Mode 3 -> FALSE ALARM -> Mode 1\")\n",
    "                    \n",
    "                    # Display FALSE ALARM\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "                    cv2.putText(frame, \"FALSE ALARM!\", (x, y - 10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)\n",
    "                    print(\"2 0 0\", flush=True)\n",
    "                    reset_to_mode1()\n",
    "            else:\n",
    "                print(\"1 0 0\", flush=True)\n",
    "                reset_to_mode1()\n",
    "\n",
    "        # ===== MODE 4: Focused FFT + Periodic Model Validation =====\n",
    "        elif mode == 4:\n",
    "            frame = convert_yuv_to_bgr(yuv_data, FRAME_W, FRAME_H)\n",
    "            frame[10:20, 260:380, :] = 0\n",
    "            \n",
    "            if bbox_mode4 is not None:\n",
    "                x, y, w, h = [int(v) for v in bbox_mode4]\n",
    "                x, y = max(0, x), max(0, y)\n",
    "                x2, y2 = min(frame_width, x + w), min(frame_height, y + h)\n",
    "                \n",
    "                # FFT check on ROI\n",
    "                roi_binary = B2[y:y2, x:x2]\n",
    "                non_zero_pixel_count = np.sum(roi_binary)\n",
    "                pixel_count_buff = np.roll(pixel_count_buff, -1)\n",
    "                pixel_count_buff[-1] = non_zero_pixel_count\n",
    "                tcounter += 1\n",
    "                \n",
    "                fft_valid = True\n",
    "                if tcounter > CALC_LENGTH:\n",
    "                    detected_freq = get_fft_frequency(pixel_count_buff, FPS)\n",
    "                    if abs(detected_freq - LED_FREQ) > FREQ_TOL:\n",
    "                        mode4_fft_failure_count += 1\n",
    "                        if mode4_fft_failure_count >= MAX_FFT_FAILURES_MODE4:\n",
    "                            logging.info(\"Mode 4: FFT frequency lost -> Mode 1\")\n",
    "                            print(\"2 0 0\", flush=True)\n",
    "                            reset_to_mode1()\n",
    "                            fft_valid = False\n",
    "                    else:\n",
    "                        mode4_fft_failure_count = 0\n",
    "                \n",
    "                if fft_valid and mode == 4:\n",
    "                    # Periodic model re-validation\n",
    "                    mode4_frame_counter += 1\n",
    "                    if mode4_frame_counter >= MODEL_VALIDATION_INTERVAL:\n",
    "                        mode4_frame_counter = 0\n",
    "                        is_detected, confidence = predict_detector(frame, bbox_mode4)\n",
    "                        logging.info(f\"Mode 4 re-validation: detected={is_detected}, conf={confidence:.3f}\")\n",
    "                        if not is_detected:\n",
    "                            logging.info(\"Mode 4: Model validation failed -> Mode 1\")\n",
    "                            print(\"2 0 0\", flush=True)\n",
    "                            reset_to_mode1()\n",
    "                    \n",
    "                    if mode == 4:  # Still in mode 4\n",
    "                        # Calculate and output alignment\n",
    "                        x_deg, y_deg, cx, cy = calculate_alignment(bbox_mode4, center_x0, center_y0)\n",
    "                        print(f\"0 {x_deg} {y_deg}\", flush=True)\n",
    "                        \n",
    "                        # Visual indication - LOCKED ON TARGET\n",
    "                        color = (0, 255, 0) if abs(x_deg) <= 0.5 and abs(y_deg) <= 0.5 else (0, 165, 255)\n",
    "                        cv2.rectangle(frame, (x, y), (x2, y2), color, 3)\n",
    "                        cv2.putText(frame, \"LOCKED ON TARGET\", (x, y - 30), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "                        cv2.putText(frame, f\"Align: ({x_deg}, {y_deg})\", (x, y - 10), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                        \n",
    "                        # Send FireBorder notification\n",
    "                        bbox_draw = [x, y, x2, y2]\n",
    "                        param1, param2 = convert_bboxes_to_reucam_params(bbox_draw)\n",
    "                        current_time = time.time()\n",
    "                        if last_fireborder != (param1, param2) or current_time - last_fire_time > 0.05:\n",
    "                            reucam.onFireBorder(0, 0)\n",
    "                            time.sleep(0.01)\n",
    "                            reucam.onFireBorder(param1, param2)\n",
    "                            last_fireborder = (param1, param2)\n",
    "                            last_fire_time = current_time\n",
    "            else:\n",
    "                print(\"1 0 0\", flush=True)\n",
    "                reset_to_mode1()\n",
    "\n",
    "        else:\n",
    "            print(\"1 0 0\", flush=True)\n",
    "\n",
    "        # Display mode and recording status\n",
    "        mode_colors = {1: (255, 255, 255), 2: (0, 255, 255), 3: (255, 0, 255), 4: (0, 255, 0)}\n",
    "        mode_names = {1: \"SCANNING\", 2: \"REFINING\", 3: \"PREDICTING\", 4: \"LOCKED\"}\n",
    "        cv2.putText(frame, f\"Mode {mode}: {mode_names.get(mode, '')}\", (40, 80),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, mode_colors.get(mode, (255,255,255)), 2)\n",
    "        cv2.putText(frame, f\"FPS: {current_fps:.1f}\", (40, 110),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "        \n",
    "        if is_recording:\n",
    "            cv2.putText(frame, f\"REC: {frame_count_record} frames\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27 or key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
